{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying research code to a compute cluster is a pain. EC2 can be particularly thorny. My workflow in past projects usually looked something like this:\n",
    "\n",
    "- Write code on a local machine\n",
    "- Manually spin up an EC2 instance\n",
    "- SSH into an instance, install the dependencies, and create an AMI\n",
    "- Spin up more instances with the new AMI, launch jobs individually\n",
    "- Manually collect output\n",
    "\n",
    "I try not to think about how many hours I've spent this way. While it's pretty obvious that most of the workflow can in principle be automated, the path to automation is long and treacherous. Most open-source projects only solve a part of the problem, and putting the pieces together often requires a fairly high level of sophistication on the part of the user.\n",
    "\n",
    "This is the part where I ought to pitch my new amazing framework that will solve all your problems with a clean, simple UI. The truth is I don't think any framework exists that gives researchers the combination of flexibility and simplicity they want. Maybe [Grid AI](https://www.grid.ai/) will get there someday. My hypothesis for why the problem of deploying research code is still so difficult is very simple. Researchers don't have the time or desire to learn frameworks. We want to write simple, transparent code that's easy to change, and then we want that same code to run on a cluster. In this post I'll take you through my process for scaling to EC2 clusters. It strikes a healthy balance between ease-of-use and flexibility. We'll be using the following packages:\n",
    "\n",
    "- [Hydra](https://hydra.cc/docs/intro/) for application configuration\n",
    "- [Ray](https://docs.ray.io/en/master/) for cluster management\n",
    "- [Docker](https://docs.docker.com/) to manage dependencies\n",
    "\n",
    "Each of the following steps will take some time, and may need to be tweaked to suit your particular situation. Think of this more as an example than a comprehensive guide. To try to keep this post to a reasonable length, I won't be including details that are well-documented, like package installation. If things don't work for you at first, keep at it! A little time invested here will save you hours of thankless grunt work later on. I've also created a [working example](https://github.com/samuelstanton/hydra-ray-demo) to make things more concrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Versions\n",
    "Hydra and Ray are in active development. Use the following commands to install Hydra and the Ray plugin.\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/facebookresearch/hydra.git@7e7832f72664b40ee04834ac8956e7146a3d37fb\n",
    "pip install git+https://github.com/facebookresearch/hydra.git@7e7832f72664b40ee04834ac8956e7146a3d37fb#subdirectory=plugins/hydra_ray_launcher\n",
    "```\n",
    "\n",
    "I've pinned the installation to a specific commit because docker is not supported in the current release. When the next stable release is made available, I will update this section. The example should work with `ray==1.2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Writing Python Scripts with Hydra (Death to argparse)\n",
    "\n",
    "I genuinely don't understand why people still use argparse, with all its boilerplate bloat. Let's take a simple example. Suppose I have the following function, `naptime`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to sleep\n",
      "waking up\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def naptime(duration):\n",
    "    print('going to sleep')\n",
    "    time.sleep(duration)\n",
    "    print('waking up')\n",
    "\n",
    "naptime(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to turn the function into a script `main.py`, and I want to be able to change the duration of the nap from the command line. Here's what you have to do with argparse:\n",
    "\n",
    "```\n",
    "# /path/to/project/main.py\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "def naptime(duration):...\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--duration', type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "    naptime(args.duration)\n",
    "```\n",
    "\n",
    "`python main.py --duration 10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all well and good. But what if I decide I want this program to be a full-blown naptime simulation, with GAN-generated dreams? Configuring such an application just from the command line will quickly become incredibly unwieldy. When I finally give in and start configuring my application with `.yaml` files, I need to add more boilerplate so that I can load the yaml config first, then process command-line overrides, and I still won't have a record of what I overrode unless I explicitly add logging boilerplate. \n",
    "\n",
    "Hydra solves all these problems, plus loads of extra functionality that make your life so much better. Here's how we'd do this with Hydra. We'd create a `config/` directory and create `config/main.yaml`.\n",
    "\n",
    "```\n",
    "# /path/to/project/config/main.yaml\n",
    "defaults:\n",
    "    - hydra/launcher: basic\n",
    "    \n",
    "duration: 1\n",
    "```\n",
    "\n",
    "The script is just a wrapper around the `runtime` function,\n",
    "\n",
    "```\n",
    "# /path/to/project/main.py\n",
    "import time\n",
    "import hydra\n",
    "\n",
    "def naptime(duration):...\n",
    "\n",
    "hydra.main(config_path='./config', config_name='main')\n",
    "def main(config):\n",
    "    naptime(config.duration)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "`python main.py duration=10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the flexibility we had before, but with automatic config and override logs, plus goodies like tab-completion if I've forgotten what flags to use. \n",
    "\n",
    "You also get Hydra's incredible composability features, which makes it easy to swap out different modules of code from the command line. I won't go into this in a lot of detail because it isn't the focus of the post, but [the documentation](https://hydra.cc/docs/tutorials/basic/your_first_app/config_groups) is great.\n",
    "\n",
    "The features I just mentioned are worth the switch on their own merits, but we're not done. Hydra _also_ includes plugins for cluster launchers (e.g. [SubmitIt](https://github.com/facebookincubator/submitit)), and sweepers like [Ax](https://ax.dev/) and [Nevergrad](https://github.com/facebookresearch/nevergrad). Crucially, the plugins are meant to work with no code changes and the same command line interface. And that's where our story really starts, with the [Hydra Ray plugin](https://hydra.cc/docs/plugins/ray_launcher)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring AWS\n",
    "\n",
    "After installing the plugin the first thing you need to do to use EC2 is make sure AWS is set up correctly. I'm going to assume you've already [set up an AWS account](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html), [requested a sufficient service quota](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-limit/), and [configured your AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). There is a lot of documentation and learning resources out there for this part, so I won't spend more time here. \n",
    "\n",
    "The Hydra Ray plugin documentation recommends setting up IAM roles for your head and worker nodes, as is done [here](https://github.com/ray-project/ray/issues/9327). This part is crucial to make sure your head node can spin up more workers, and to make sure both the head and worker nodes can access S3.\n",
    "\n",
    "In addition to using EC2 for compute resources, you can also use S3 to store output. I wrote a [simple dataframe logger](https://github.com/samuelstanton/upcycle/blob/master/upcycle/logging/s3_logger.py) that writes directly to S3, as long as your AWS credentials are configured. At the end we can quickly pull all of our output to our local machine by running \n",
    "\n",
    "`aws s3 sync s3://my-bucket/path/to/remote/ path/to/project/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Containerizing your code\n",
    "\n",
    "For a simple program like `main.py`, containerization is obviously overkill. In practice ML research projects tend to have many dependencies, both at the system level (e.g. CUDA) and at the python environment level (PyTorch, Pandas, Torchvision, etc.). There is also typically dependency on source code that only exists in git repos. For many of us, retracing the steps we took to create a local compute environment is painful and time-consuming. Containerization adds some complexity upfront, but it ensures a stable environment from your code, free from the vagaries of package version changes and cluster system updates. To get started, you can try using [the image I've created](https://hub.docker.com/repository/docker/samuelstanton/hydra-ray-demo). Keep reading if you want\n",
    "to know how to make your own image.\n",
    "\n",
    "You'll add a Dockerfile to the working directory that looks something like [the one from my github example](https://github.com/samuelstanton/hydra-ray-demo/blob/main/Dockerfile),\n",
    "\n",
    "```\n",
    "# /path/to/project/Dockerfile\n",
    "FROM ubuntu:18.04\n",
    "\n",
    "# Ray wants these lines\n",
    "ENV LC_ALL=C.UTF-8\n",
    "ENV LANG=C.UTF-8\n",
    "\n",
    "RUN apt update && apt install software-properties-common -y\n",
    "# python3.8-dev includes headers that are needed to install pickle5 later\n",
    "RUN apt-get install python3.8-dev gcc -y\n",
    "# install python 3.8 virtual environment\n",
    "RUN add-apt-repository ppa:deadsnakes/ppa\n",
    "RUN apt update && apt install python3.8-venv git -y\n",
    "ENV VIRTUAL_ENV=/opt/venv\n",
    "RUN python3.8 -m venv $VIRTUAL_ENV\n",
    "ENV PATH=$VIRTUAL_ENV/bin:$PATH\n",
    "RUN python -m pip install --upgrade pip setuptools\n",
    "\n",
    "# install java, requirement to install hydra from source\n",
    "RUN apt install default-jre -y\n",
    "\n",
    "RUN mkdir src\n",
    "COPY hydra-ray-demo/ src/hydra-ray-demo/\n",
    "RUN python -m pip install -r src/hydra-ray-demo/requirements.txt\n",
    "WORKDIR src/hydra-ray-demo\n",
    "```\n",
    "\n",
    "There's a couple things to note about this Dockerfile. First, even though this is a container, we're still using a virtual python environment (and it's not conda). The reason for keeping the virtual python environment is it allows a clean separation between system-level packages managed with `apt` and environment-level packages managed with `pip`. We aren't using conda because Docker and conda don't play very nicely together. For conda to work as intended, you need to source your `.bashrc` and `.bash_profile` before running anything. This happens automatically if you start an interactive bash session, or use `/bin/bash --login -c `. The default shell for Docker is `/bin/sh`, which does _not_ source those files. You can always hack the `PATH` environment variable, \n",
    "but that won't necessarily replicate all the behavior of `conda activate`. Although I did initially try to use conda eventually I wound up using virtualenv instead. As a bonus, [here's a gist](https://gist.github.com/samuelstanton/65a0a6855d6f3c4943164968c1310132) of how to write a Dockerfile that installs packages in a conda virtual environment, if you're set on using it.\n",
    "\n",
    "\n",
    "To build your Docker container you need to be outside the project directory. Once we've built the image, we'll push it to [DockerHub](https://hub.docker.com/) to use later. \n",
    "```\n",
    "cd /path/to/project\n",
    "cd ..\n",
    "docker build . -f /path/to/project/Dockerfile --tag <DOCKERHUB_USER>/<IMAGE_NAME>:<TAG>\n",
    "docker push <DOCKERHUB_USER>/<IMAGE_NAME>:<TAG>\n",
    "```\n",
    "\n",
    "Note: Keep the contents of the parent directory to a minimum for fastest build times.\n",
    "\n",
    "After using docker for a while your storage will be quickly filled with fragments of previous images.\n",
    "Use `docker system prune -f` to clean that up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuring the Hydra Ray Plugin\n",
    "\n",
    "Now we need to update our program config to accomodate the Hydra Ray launcher plugin. In order to maintain our ability to run our program locally, we'll make use of Hydra's composability and add another file, `config/hydra/launcher/ray_aws.yaml`\n",
    "\n",
    "\n",
    "```\n",
    "# /path/to/project/config/hydra/launcher/ray_aws.yaml\n",
    "_target_: hydra_plugins.hydra_ray_launcher.ray_aws_launcher.RayAWSLauncher\n",
    "env_setup:\n",
    "  pip_packages:\n",
    "    omegaconf: null\n",
    "    hydra_core: null\n",
    "    ray: null\n",
    "    cloudpickle: null\n",
    "    pickle5: null\n",
    "    hydra_ray_launcher: null\n",
    "  commands: []\n",
    "ray:\n",
    "  cluster:\n",
    "    cluster_name: demo-cluster\n",
    "    min_workers: 0\n",
    "    max_workers: 0\n",
    "    initial_workers: 0\n",
    "    autoscaling_mode: default\n",
    "    target_utilization_fraction: 0.8\n",
    "    idle_timeout_minutes: 5\n",
    "    docker:\n",
    "      image: 'samuelstanton/hydra-ray-demo:latest'\n",
    "      container_name: 'hydra-container'\n",
    "      pull_before_run: true\n",
    "      run_options: []\n",
    "    provider:\n",
    "      type: aws\n",
    "      region: us-east-2\n",
    "      availability_zone: us-east-2a,us-east-2b\n",
    "    auth:\n",
    "      ssh_user: ubuntu\n",
    "    head_node:\n",
    "      InstanceType: m4.large\n",
    "      ImageId: ami-010bc10395b6826fb\n",
    "    worker_nodes:\n",
    "      InstanceType: m4.large\n",
    "      ImageId: ami-010bc10395b6826fb\n",
    "stop_cluster: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Profit.\n",
    "\n",
    "At last we have arrived at our destination. If you recall, the command we used to run our program locally was `python main.py duration=10`. You can still run that command, and the program's behavior will be unchanged. To simultaneously excute several realizations of your program on EC2, simply use\n",
    "`python main.py -m hydra/launcher=ray_aws duration=1,2,4,8`, and you're off to the races!\n",
    "\n",
    "If you've stayed with me this long, thanks for reading! I hope you find this useful. I arrived at this procedure mostly through trial-and-error, so don't be afraid to try to improve on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources:\n",
    "- [Hydra docs](https://hydra.cc/docs/intro)\n",
    "- [Example on GitHub](https://github.com/samuelstanton/hydra-ray-demo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nikola": {
   "date": "2021-03-05 03:54:00 UTC",
   "slug": "deploying-research-code",
   "tags": "aws, ec2, hydra, ray, docker, research, ML, AI",
   "title": "Deploying Research Code with Hydra, Ray, and Docker"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
